import os
import requests

from llama_index.core import (
    Document,
    StorageContext,
    VectorStoreIndex,
    Settings,
    load_index_from_storage
)
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
OUTLINE_API_KEY = os.environ["OUTLINE_API_KEY"]
OUTLINE_API_URL = os.environ["OUTLINE_API_URL"]
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")


# === –ó–ê–ì–†–£–ó–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í –ò–ó –í–°–ï–• –ö–û–õ–õ–ï–ö–¶–ò–ô OUTLINE ===
def fetch_outline_documents():
    docs = []
    headers = {
        "Authorization": f"Bearer {OUTLINE_API_KEY}",
        "Content-Type": "application/json"
    }

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–ª–µ–∫—Ü–∏–π
    print("üìÅ –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–ª–µ–∫—Ü–∏–π...")
    collections_resp = requests.post(
        f"{OUTLINE_API_URL}/collections.list",
        headers=headers
    )

    collections = collections_resp.json().get("data", [])
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")

    for col in collections:
        collection_id = col["id"]
        print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é: {col['name']} ({collection_id})")
        page = 1
        while True:
            response = requests.post(
                f"{OUTLINE_API_URL}/documents.list",
                headers=headers,
                json={"collectionId": collection_id, "limit": 100, "offset": (page - 1) * 100}
            )
            if response.status_code != 200:
                print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:", response.text)
                break

            results = response.json().get("data", [])
            if not results:
                break

            for doc in results:
                if doc.get("archived") or not doc.get("text"):
                    continue
                docs.append(Document(
                    text=doc["text"],
                    metadata={"title": doc["title"], "url": doc.get("url", "")}
                ))

            page += 1

    return docs


# === –°–û–ó–î–ê–ù–ò–ï –ò–õ–ò –û–ë–ù–û–í–õ–ï–ù–ò–ï –ò–ù–î–ï–ö–°–ê ===
def build_or_update_index(docs):
    persist_dir = "./storage"
    try:
        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
        index = load_index_from_storage(storage_context)
        print("üì¶ –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω. –û–±–Ω–æ–≤–ª—è–µ–º...")
        index.insert_documents(docs)
        index.storage_context.persist(persist_dir=persist_dir)
    except Exception as e:
        print("üì¶ –ò–Ω–¥–µ–∫—Å–∞ –Ω–µ—Ç. –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π...")
        index = VectorStoreIndex.from_documents(docs)
        index.storage_context.persist(persist_dir=persist_dir)

    print("‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω")


# === –¢–û–ß–ö–ê –í–•–û–î–ê ===
if __name__ == "__main__":
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Outline...")
    documents = fetch_outline_documents()
    print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

    if documents:
        print("‚öôÔ∏è –û–±–Ω–æ–≤–ª—è–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å...")
        build_or_update_index(documents)
    else:
        print("‚ùå –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞.")


