import os
import requests
from llama_index import VectorStoreIndex, Document
from llama_index.llms import OpenAI
from llama_index.embeddings import OpenAIEmbedding
from llama_index import ServiceContext

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
OUTLINE_API_KEY = ""
OUTLINE_API_URL = ""
OPENAI_API_KEY = ""

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# === –ó–ê–ì–†–£–ó–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í –ò–ó OUTLINE ===
def fetch_outline_documents():
    docs = []
    page = 1
    while True:
        response = requests.post(
            OUTLINE_API_URL,
            headers={"Authorization": f"Bearer {OUTLINE_API_KEY}"},
            json={"limit": 100, "offset": (page - 1) * 100}
        )
        if response.status_code != 200:
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:", response.text)
            break

        data = response.json()
        results = data.get("data", [])
        if not results:
            break

        for doc in results:
            if doc.get("archived") or not doc.get("text"):
                continue
            docs.append(Document(
                text=doc["text"],
                metadata={"title": doc["title"]}
            ))

        page += 1
    return docs

# === –°–û–ó–î–ê–ù–ò–ï –ò–ù–î–ï–ö–°–ê ===
def build_index(docs):
    service_context = ServiceContext.from_defaults(
        llm=OpenAI(temperature=0),
        embed_model=OpenAIEmbedding()
    )
    index = VectorStoreIndex.from_documents(docs, service_context=service_context)
    index.storage_context.persist(persist_dir="./storage")
    return index

# === –û–°–ù–û–í–ù–ê–Ø –¢–û–ß–ö–ê –í–•–û–î–ê ===
if __name__ == "__main__":
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Outline...")
    documents = fetch_outline_documents()
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

    print("‚öôÔ∏è –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å...")
    build_index(documents)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ ./storage")
