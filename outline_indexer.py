import os
import requests
from llama_index.core import VectorStoreIndex, Document, StorageContext, Settings
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

# ======= –ù–∞—Å—Ç—Ä–æ–π–∫–∏ =======
OUTLINE_API_URL = "https://outline.taliaslimbot.com/api"
OUTLINE_API_KEY = "ol_"
OPENAI_API_KEY = "sk-"

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
HEADERS = {"Authorization": f"Bearer {OUTLINE_API_KEY}"}

# ======= –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π =======
def fetch_all_documents():
    print("üìÅ –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–ª–µ–∫—Ü–∏–π...")
    collections = requests.post(
        f"{OUTLINE_API_URL}/collections.list",
        headers=HEADERS
    ).json().get("data", [])
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections)}")

    all_docs = []

    for col in collections:
        col_id = col["id"]
        col_name = col["name"]
        print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é: {col_name} ({col_id})")

        page = 0
        while True:
            resp = requests.post(
                f"{OUTLINE_API_URL}/documents.list",
                headers=HEADERS,
                json={"collectionId": col_id, "limit": 100, "offset": page * 100}
            )
            results = resp.json().get("data", [])
            if not results:
                break

            for doc in results:
                doc_id = doc["id"]
                title = doc["title"]

                export_resp = requests.post(
                    f"{OUTLINE_API_URL}/documents.export",
                    headers=HEADERS,
                    json={"id": doc_id}
                )
                if export_resp.status_code != 200:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {title}")
                    continue

                text = export_resp.json().get("data", "").strip()
                if not text:
                    print(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –ø—É—Å—Ç–æ–π: {title}")
                    continue

                all_docs.append(Document(
                    text=text,
                    metadata={"title": title, "collection": col_name}
                ))

            page += 1

    print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_docs)}")
    return all_docs

# ======= –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ =======
def build_index(docs):
    Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
    Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
    index = VectorStoreIndex.from_documents(docs)
    index.storage_context.persist(persist_dir="./storage")
    return index

# ======= –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ =======
if __name__ == "__main__":
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Outline...")
    documents = fetch_all_documents()

    if documents:
        print("‚öôÔ∏è –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å...")
        build_index(documents)
        print("‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ ./storage")
    else:
        print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")



