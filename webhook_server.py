import os
from fastapi import FastAPI, Request
from pydantic import BaseModel
from llama_index import StorageContext, load_index_from_storage
from llama_index.llms import OpenAI
from llama_index.embeddings import OpenAIEmbedding
from llama_index import ServiceContext
import uvicorn

# ==== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ====
os.environ["OPENAI_API_KEY"] = ""

# ==== FastAPI init ====
app = FastAPI()

# ==== –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ ====
print("üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å...")
storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
service_context = ServiceContext.from_defaults(
    llm=OpenAI(temperature=0),
    embed_model=OpenAIEmbedding()
)
query_engine = index.as_query_engine(service_context=service_context)

# ==== –ú–æ–¥–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞ ====
class AskRequest(BaseModel):
    question: str

# ==== –í–µ–±—Ö—É–∫ ====
@app.post("/ask")
async def ask(request: AskRequest):
    try:
        response = query_engine.query(request.question)
        return {"answer": str(response)}
    except Exception as e:
        return {"error": str(e)}

# ==== –ó–∞–ø—É—Å–∫ ====
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
