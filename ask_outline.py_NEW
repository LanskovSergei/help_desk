import os
from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional
import uvicorn
from datetime import datetime

from llama_index.core import load_index_from_storage, Settings
from llama_index.core.storage.storage_context import StorageContext
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# ==== Config ====
os.environ["OPENAI_API_KEY"] = "sk-proj-..."  # <-- Укажи свой ключ здесь
ALLOWED_USER_IDS = {"123456789"}  # <-- Укажи список разрешённых user_id

Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# ==== Load Index ====
storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
query_engine = index.as_query_engine(similarity_top_k=3)

# ==== FastAPI App ====
app = FastAPI()

# ==== Request/Response Models ====
class QuestionRequest(BaseModel):
    question: str
    user_id: Optional[str] = None

class AIResponse(BaseModel):
    answer: str
    article_url: Optional[str] = None
    has_answer: bool

# ==== Main Endpoint ====
@app.post("/ask", response_model=AIResponse)
async def ask_ai(request: QuestionRequest):
    # Log request
    log_line = f"{datetime.now().isoformat()} | USER_ID: {request.user_id} | Q: {request.question}\n"
    with open("requests.log", "a") as f:
        f.write(log_line)

    # Access control
    if request.user_id not in ALLOWED_USER_IDS:
        return AIResponse(
            answer="Access denied. You are not authorized to use this service.",
            article_url=None,
            has_answer=False
        )

    try:
        response = query_engine.query(request.question)

        # Check: any relevant source nodes?
        if not response.source_nodes:
            return AIResponse(
                answer="Information not found. Would you like to talk to an operator?",
                article_url=None,
                has_answer=False
            )

        # Extract response and source text
        answer_text = str(response).strip()
        first_node = response.source_nodes[0]
        source_text = first_node.node.text.strip().lower()

        # Validate answer quality
        if (
            not answer_text
            or "i'm here to help" in answer_text.lower()
            or answer_text.lower() == source_text
            or len(answer_text.split()) < 5
        ):
            return AIResponse(
                answer="Information not found. Would you like to talk to an operator?",
                article_url=None,
                has_answer=False
            )

        article_url = first_node.node.metadata.get("url")

        return AIResponse(
            answer=answer_text,
            article_url=article_url,
            has_answer=True
        )

    except Exception:
        return AIResponse(
            answer="Information not found. Would you like to talk to an operator?",
            article_url=None,
            has_answer=False
        )

# ==== Server Runner ====
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

